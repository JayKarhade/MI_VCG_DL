{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MI_Localization_LSOC.ipynb",
      "provenance": [],
      "mount_file_id": "1auec5G3Y47rgKXaydCFQg7XOqIEDeOAc",
      "authorship_tag": "ABX9TyPLLfpBZIXtz7UQPnlTCDRS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayKarhade/MI_VCG_DL/blob/main/MI_Localization_LSOC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_P986gbUzUR"
      },
      "source": [
        "import os \n",
        "import glob \n",
        "import PIL\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as sio \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings \n",
        "import cv2\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Input, Dense, Activation, Flatten, Dropout,BatchNormalization\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.optimizers import SGD,Adam\n",
        "from keras.utils import np_utils\n",
        "from sklearn.utils import shuffle\n",
        "from PIL import Image\n",
        "from tensorflow.keras.regularizers import l2\n",
        "#from keras.utils import to_categorical\n",
        "from scipy.io import loadmat\n",
        "from keras import layers\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIGHNIfpVBJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef3a57fe-c09b-418c-e21d-8258c8c5b899"
      },
      "source": [
        "path1 = \"/content/drive/MyDrive/localization_mi/anterior_mi\"\n",
        "path2 = \"/content/drive/MyDrive/localization_mi/anterolateral_mi\"\n",
        "path3 = \"/content/drive/MyDrive/localization_mi/anteroseptal_mi\"\n",
        "path4 = \"/content/drive/MyDrive/localization_mi/inferior_mi\"\n",
        "path5 = \"/content/drive/MyDrive/localization_mi/inferolateral_mi\"\n",
        "path6 = \"/content/drive/MyDrive/localization_mi/infero_postero_lateral_mi\"\n",
        "my_list_class1 = os.listdir(path1)\n",
        "my_list_class2 = os.listdir(path2)\n",
        "my_list_class3 = os.listdir(path3)\n",
        "my_list_class4 = os.listdir(path4)\n",
        "my_list_class5 = os.listdir(path5)\n",
        "my_list_class6 = os.listdir(path6)\n",
        "\n",
        "data_load = []\n",
        "sum = 0\n",
        "y = []\n",
        "\n",
        "instance_counter = []\n",
        "curr_count = 0\n",
        "for i in range(len(my_list_class1)):\n",
        "  address = path1+'/'+my_list_class1[i]\n",
        "  temp_data = np.asarray(loadmat(address)['chm'].T)##Get each file content\n",
        "  curr_count = curr_count + temp_data.shape[0]\n",
        "  instance_counter.append(curr_count)\n",
        "  for j in range(temp_data.shape[0]):\n",
        "    data_load.append(temp_data[j])##Append all instances in each .mat file to data_load\n",
        "    y.append(0)##Class Label\n",
        "  sum = sum + temp_data.shape[0]##Debugging step to make sure all instances are done\n",
        "\n",
        "for i in range(len(my_list_class2)):\n",
        "  address = path2+'/'+my_list_class2[i]\n",
        "  temp_data = np.asarray(loadmat(address)['chm'].T)##Get each file content\n",
        "  curr_count = curr_count + temp_data.shape[0]\n",
        "  instance_counter.append(curr_count)\n",
        "  for j in range(temp_data.shape[0]):\n",
        "    data_load.append(temp_data[j])##Append all instances in each .mat file to data_load\n",
        "    y.append(1)##Class Label\n",
        "  sum = sum + temp_data.shape[0]##Debugging step to make sure all instances are done\n",
        "\n",
        "for i in range(len(my_list_class3)):\n",
        "  address = path3+'/'+my_list_class3[i]\n",
        "  temp_data = np.asarray(loadmat(address)['chm'].T)##Get each file content\n",
        "  curr_count = curr_count + temp_data.shape[0]\n",
        "  instance_counter.append(curr_count)\n",
        "  for j in range(temp_data.shape[0]):\n",
        "    data_load.append(temp_data[j])##Append all instances in each .mat file to data_load\n",
        "    y.append(2)##Class Label\n",
        "  sum = sum + temp_data.shape[0]##Debugging step to make sure all instances are done\n",
        "\n",
        "for i in range(len(my_list_class4)):\n",
        "  address = path4+'/'+my_list_class4[i]\n",
        "  temp_data = np.asarray(loadmat(address)['chm'].T)##Get each file content\n",
        "  curr_count = curr_count + temp_data.shape[0]\n",
        "  instance_counter.append(curr_count)\n",
        "  for j in range(temp_data.shape[0]):\n",
        "    data_load.append(temp_data[j])##Append all instances in each .mat file to data_load\n",
        "    y.append(3)##Class Label\n",
        "  sum = sum + temp_data.shape[0]##Debugging step to make sure all instances are done\n",
        "\n",
        "for i in range(len(my_list_class5)):\n",
        "  address = path5+'/'+my_list_class5[i]\n",
        "  temp_data = np.asarray(loadmat(address)['chm'].T)##Get each file content\n",
        "  curr_count = curr_count + temp_data.shape[0]\n",
        "  instance_counter.append(curr_count)\n",
        "  for j in range(temp_data.shape[0]):\n",
        "    data_load.append(temp_data[j])##Append all instances in each .mat file to data_load\n",
        "    y.append(4)##Class Label\n",
        "  sum = sum + temp_data.shape[0]##Debugging step to make sure all instances are done\n",
        "\n",
        "for i in range(len(my_list_class6)):\n",
        "  address = path6+'/'+my_list_class6[i]\n",
        "  temp_data = np.asarray(loadmat(address)['chm'].T)##Get each file content\n",
        "  curr_count = curr_count + temp_data.shape[0]\n",
        "  instance_counter.append(curr_count)\n",
        "  for j in range(temp_data.shape[0]):\n",
        "    data_load.append(temp_data[j])##Append all instances in each .mat file to data_load\n",
        "    y.append(5)##Class Label\n",
        "  sum = sum + temp_data.shape[0]##Debugging step to make sure all instances are done\n",
        "\n",
        "\n",
        "\n",
        "data_load = np.asarray(data_load)\n",
        "y = np.asarray(y)\n",
        "print(y.shape)\n",
        "print(data_load.shape)\n",
        "\n",
        "dataset = np.zeros([14336,15,651])\n",
        "\n",
        "for i in range(14336):\n",
        "  for j in range(3):\n",
        "    for k in range(5):\n",
        "      dataset[i,(5*j) + k] = data_load[i][j][k]\n",
        "\n",
        "data = np.zeros([14336,651,15])\n",
        "for i in range(14336):\n",
        "  data[i] = dataset[i].T\n",
        "\n",
        "#np.save(\"/content/drive/MyDrive/MI_Files/MI_loc_label.npy\",y)\n",
        "#np.save(\"/content/drive/MyDrive/MI_Files/MI_loc_data.npy\",data)\n",
        "print(instance_counter)\n",
        "np.save('/content/drive/MyDrive/MI_Files/localization_instances_indices.npy',np.asarray(instance_counter))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14336,)\n",
            "(14336, 3)\n",
            "[151, 313, 457, 622, 775, 888, 991, 1086, 1233, 1422, 1545, 1664, 1818, 1969, 2127, 2253, 2404, 2525, 2662, 2933, 3151, 3299, 3442, 3583, 3696, 3838, 3979, 4118, 4288, 4433, 4559, 4758, 4915, 5105, 5260, 5368, 5540, 5685, 5839, 5993, 6120, 6237, 6378, 6499, 6538, 6682, 6806, 6931, 7061, 7214, 7325, 7469, 7569, 7711, 7921, 8075, 8276, 8403, 8580, 8754, 8872, 8989, 9157, 9306, 9525, 9703, 9818, 9976, 10129, 10250, 10455, 10612, 10740, 10860, 11009, 11167, 11329, 11455, 11580, 11707, 11846, 11964, 12111, 12245, 12355, 12482, 12620, 12754, 12928, 13110, 13221, 13382, 13512, 13675, 13852, 13969, 14156, 14336]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rCO_Z7DWdYx"
      },
      "source": [
        "**Run from here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV2ovYJYIwb6",
        "outputId": "18dbea0c-14e5-4a1e-eb0f-f8f3eab2d20a"
      },
      "source": [
        "instance_counter = np.load('/content/drive/MyDrive/MI_Files/localization_instances_indices.npy')\n",
        "instance_counter.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqTjfJ10WfDH"
      },
      "source": [
        "data = np.load(\"/content/drive/MyDrive/localization_mi/MIdata.npy\")\n",
        "y = np.load(\"/content/drive/MyDrive/localization_mi/MI_label.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivwPr3EEWhYv"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(data, y, test_size=0.1, random_state=1)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/8, random_state=1)\n",
        "\n",
        "\n",
        "y_tr_one_hot  = np.zeros((np.array(y_train).shape[0],6))\n",
        "\n",
        "for i in range(np.array(y_train).shape[0]):\n",
        "  label = y_train[i]\n",
        "  y_tr_one_hot[i][int(label)] = 1\n",
        "\n",
        "y_val_one_hot  = np.zeros((np.array(y_val).shape[0],6))\n",
        "\n",
        "for i in range(np.array(y_val).shape[0]):\n",
        "  label = y_val[i]\n",
        "  y_val_one_hot[i][int(label)] = 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ww4cDvNWjN6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "80480750-4867-4a99-e869-259cf3148ebe"
      },
      "source": [
        "##Define model\n",
        "def model_define():\n",
        "  model=Sequential()\n",
        "  model.add(Conv1D(filters=20, kernel_size=5, activation='relu',input_shape=(651,15)))\n",
        "  model.add(MaxPooling1D(pool_size=3, strides = 3))\n",
        "  model.add(Conv1D(filters=60, kernel_size=5, activation='relu'))\n",
        "  #model.add(Dropout(0.7))\n",
        "  model.add(MaxPooling1D(pool_size=3, strides = 5))\n",
        "  model.add(Conv1D(filters=120, kernel_size=5,))\n",
        "  model.add(Conv1D(filters=120,kernel_size=7,))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(2000, activation='relu'))\n",
        "  model.add(Dense(700, activation='relu'))\n",
        "  model.add(Dense(50,activation='relu'))\n",
        "  model.add(Dense(6,activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "k_sum = 0\n",
        "f1_sum = 0\n",
        "acc_sum = 0\n",
        "sens_sum = 0\n",
        "spec_sum = 0\n",
        "\n",
        "counter_len = instance_counter.shape[0]\n",
        "acc_list = []\n",
        "for i in range(counter_len):\n",
        "\n",
        "  subject_break = i\n",
        "  print(subject_break)\n",
        "  data = np.concatenate((np.load(\"/content/drive/MyDrive/localization_mi/MI_loc_data.npy\")[:instance_counter[subject_break]],np.load(\"/content/drive/MyDrive/localization_mi/MI_loc_data.npy\")[instance_counter[subject_break+1]:]))\n",
        "  y = np.concatenate((np.load(\"/content/drive/MyDrive/localization_mi/MI_loc_label.npy\")[:instance_counter[subject_break]],np.load(\"/content/drive/MyDrive/localization_mi/MI_loc_label.npy\")[instance_counter[subject_break+1]:]))\n",
        "\n",
        "  x_test = np.load(\"/content/drive/MyDrive/localization_mi/MI_loc_data.npy\")[instance_counter[subject_break]:instance_counter[subject_break+1]]\n",
        "  y_test = np.load(\"/content/drive/MyDrive/localization_mi/MI_loc_label.npy\")[instance_counter[subject_break]:instance_counter[subject_break+1]]\n",
        "\n",
        "  x_train, x_val, y_train, y_val = train_test_split(data, y, test_size=0.1, random_state=1)\n",
        "  y_tr_one_hot  = np.zeros((np.array(y_train).shape[0],6))\n",
        "\n",
        "  for i in range(np.array(y_train).shape[0]):\n",
        "    label = y_train[i]\n",
        "    y_tr_one_hot[i][int(label)] = 1\n",
        "\n",
        "  y_val_one_hot  = np.zeros((np.array(y_val).shape[0],6))\n",
        "\n",
        "  for i in range(np.array(y_val).shape[0]):\n",
        "    label = y_val[i]\n",
        "    y_val_one_hot[i][int(label)] = 1\n",
        "\n",
        "  K.clear_session()\n",
        "  modelf = model_define()\n",
        "\n",
        "  #print(modelf.summary())\n",
        "  optimizer = keras.optimizers.Adam(lr=0.00004)\n",
        "  modelf.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  modelf.fit(np.array(x_train), y_tr_one_hot, validation_data=(np.array(x_val), y_val_one_hot), epochs=15, batch_size=1024)\n",
        "\n",
        "  y_te_one_hot  = np.zeros((np.array(y_test).shape[0],6))\n",
        "\n",
        "  for i in range(np.array(y_test).shape[0]):\n",
        "    label = y_test[i]\n",
        "    y_te_one_hot[i][int(label)] = 1\n",
        "\n",
        "  test_loss, test_acc = modelf.evaluate(np.array(x_test), np.array(y_te_one_hot), verbose=0)\n",
        "  print('Accuracy:',test_acc)\n",
        "  acc_sum = acc_sum + test_acc\n",
        "\n",
        "  ##Evaluating Sensitivity, Accuracy and Kappa scores\n",
        "\n",
        "  Y_pred = modelf.predict_classes(x_test)\n",
        "  \n",
        "  cm1 = confusion_matrix(y_test,Y_pred)\n",
        "  print(\"confusion matrix \\n\",cm1)\n",
        "\n",
        "  K_cappa = sklearn.metrics.cohen_kappa_score(y_test,Y_pred)\n",
        "  k_sum = k_sum+K_cappa\n",
        "  print(\"cohen kappa scores:\" ,K_cappa)\n",
        "\n",
        "  F1 = sklearn.metrics.f1_score(y_test,Y_pred,average=\"micro\")\n",
        "  f1_sum = f1_sum + F1\n",
        "  print(\"F1 scores:\", F1)\n",
        "\n",
        "  #sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "  #sens_sum = sens_sum + sensitivity1\n",
        "  #print('Sensitivity : ', sensitivity1 )\n",
        "\n",
        "  #specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "  #spec_sum = spec_sum + specificity1\n",
        "  #print('Specificity : ', specificity1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "13/13 [==============================] - 36s 3s/step - loss: 1.6788 - accuracy: 0.3073 - val_loss: 1.0902 - val_accuracy: 0.5769\n",
            "Epoch 2/15\n",
            "13/13 [==============================] - 37s 3s/step - loss: 0.9235 - accuracy: 0.6574 - val_loss: 0.4877 - val_accuracy: 0.8498\n",
            "Epoch 3/15\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.4202 - accuracy: 0.8697 - val_loss: 0.2384 - val_accuracy: 0.9344\n",
            "Epoch 4/15\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.2132 - accuracy: 0.9348 - val_loss: 0.1578 - val_accuracy: 0.9542\n",
            "Epoch 5/15\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.1196 - accuracy: 0.9656 - val_loss: 0.0865 - val_accuracy: 0.9810\n",
            "Epoch 6/15\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.0541 - accuracy: 0.9874 - val_loss: 0.0448 - val_accuracy: 0.9965\n",
            "Epoch 7/15\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.0229 - accuracy: 0.9945 - val_loss: 0.0321 - val_accuracy: 0.9993\n",
            "Epoch 8/15\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.0111 - accuracy: 0.9983 - val_loss: 0.0337 - val_accuracy: 0.9965\n",
            "Epoch 9/15\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.0061 - accuracy: 0.9991 - val_loss: 0.0279 - val_accuracy: 0.9993\n",
            "Epoch 10/15\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0268 - val_accuracy: 0.9993\n",
            "Epoch 11/15\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.0274 - val_accuracy: 0.9993\n",
            "Epoch 12/15\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0307 - val_accuracy: 0.9986\n",
            "Epoch 13/15\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 0.9993\n",
            "Epoch 14/15\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.0294 - val_accuracy: 0.9993\n",
            "Epoch 15/15\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0301 - val_accuracy: 0.9986\n",
            "Accuracy: 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "confusion matrix \n",
            " [[162]]\n",
            "cohen kappa scores: nan\n",
            "F1 scores: 1.0\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:604: RuntimeWarning: invalid value encountered in true_divide\n",
            "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "13/13 [==============================] - 36s 3s/step - loss: 1.6613 - accuracy: 0.2752 - val_loss: 1.1187 - val_accuracy: 0.5761\n",
            "Epoch 2/15\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.9373 - accuracy: 0.6459 - val_loss: 0.5521 - val_accuracy: 0.8077\n",
            "Epoch 3/15\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.4509 - accuracy: 0.8454 - val_loss: 0.2408 - val_accuracy: 0.9261\n",
            "Epoch 4/15\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.1978 - accuracy: 0.9397 - val_loss: 0.1137 - val_accuracy: 0.9739\n",
            "Epoch 5/15\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.0878 - accuracy: 0.9763 - val_loss: 0.0644 - val_accuracy: 0.9859\n",
            "Epoch 6/15\n",
            " 7/13 [===============>..............] - ETA: 15s - loss: 0.0401 - accuracy: 0.9901"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4ef567e51eae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00004\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mmodelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0mmodelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0my_te_one_hot\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iquRgT5HWlff"
      },
      "source": [
        "print(\"Average holdout accuracy score :\",acc_sum/5)\n",
        "print(\"Average holdout kappa score :\",k_sum/5)\n",
        "print(\"Average holdout f1 score :\",f1_sum/5)\n",
        "print(\"Average holdout sensitivity score :\",sens_sum/5)\n",
        "print(\"Average holdout specificity score :\",spec_sum/5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Fup3i5WoTG"
      },
      "source": [
        "**K fold Cross Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH1zeZwdWq53"
      },
      "source": [
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data_cv = data[indices]\n",
        "y_cv = y[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84X3vWAUWsrP"
      },
      "source": [
        "y_one_hot  = np.zeros((np.array(y_cv).shape[0],6))\n",
        "\n",
        "for i in range(np.array(y_cv).shape[0]):\n",
        "  label = y_cv[i]\n",
        "  y_one_hot[i][int(label)] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkd1AiuRWuYm"
      },
      "source": [
        "##Cross Validation\n",
        "folds=10\n",
        "k_cv_sum = 0\n",
        "f1_cv_sum = 0\n",
        "acc_cv_sum = 0\n",
        "sens_cv_sum = 0\n",
        "spec_cv_sum = 0\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1)\n",
        "#cvscores = []\n",
        "\n",
        "for train,test in kfold.split(data_cv,y_cv):\n",
        "  model=Sequential()\n",
        "  model.add(Conv1D(filters=20, kernel_size=5, activation='relu',input_shape=(651,15)))\n",
        "  model.add(MaxPooling1D(pool_size=3, strides = 3))\n",
        "  model.add(Conv1D(filters=60, kernel_size=5, activation='relu'))\n",
        "  #model.add(Dropout(0.7))\n",
        "  model.add(MaxPooling1D(pool_size=3, strides = 5))\n",
        "  model.add(Conv1D(filters=120, kernel_size=5,))\n",
        "  model.add(Conv1D(filters=120,kernel_size=7,))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(2000, activation='relu'))\n",
        "  model.add(Dense(700, activation='relu'))\n",
        "  model.add(Dense(50,activation='relu'))\n",
        "  model.add(Dense(6,activation='sigmoid'))\n",
        "  \n",
        "  opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  #Train\n",
        "  model.fit(x_cv[train], y_one_hot[train],batch_size=256,epochs=15,verbose=1)\n",
        "  #Test\n",
        "  test_loss, test_acc = model.evaluate(data_cv[test],y_one_hot[test],batch_size=8, verbose=1)\n",
        "  #Metrics\n",
        "  print('Accuracy:',test_acc)\n",
        "  acc_cv_sum = acc_cv_sum + test_acc\n",
        "  Y_pred = model.predict_classes(data_cv[test])\n",
        "\n",
        "  K_cappa = sklearn.metrics.cohen_kappa_score(y_cv[test],Y_pred)\n",
        "  k_cv_sum = k_cv_sum+K_cappa\n",
        "  print(\"cohen kappa scores:\" ,K_cappa)\n",
        "\n",
        "  F1 = sklearn.metrics.f1_score(y_cv[test],Y_pred,average=\"micro\")\n",
        "  f1_cv_sum = f1_cv_sum + F1\n",
        "  print(\"F1 scores:\", F1)\n",
        "\n",
        "  cm1 = confusion_matrix(y_cv[test],Y_pred)\n",
        "  print(\"confusion matrix \\n\",cm1)\n",
        "\n",
        "  sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "  sens_cv_sum = sens_cv_sum + sensitivity1\n",
        "  print('Sensitivity : ', sensitivity1 )\n",
        "\n",
        "  specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "  spec_cv_sum = spec_cv_sum + specificity1\n",
        "  print('Specificity : ', specificity1)\n",
        "\n",
        "  #cvscores.append(test_acc)\n",
        "  K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN2vrrCCW0_J"
      },
      "source": [
        "print(\"Average 10 cv  accuracy score :\",acc_cv_sum/10)\n",
        "print(\"Average 10 cv  kappa score :\",k_cv_sum/10)\n",
        "print(\"Average 10 cv  f1 score :\",f1_cv_sum/10)\n",
        "print(\"Average 10 cv  sensitivity score :\",sens_cv_sum/10)\n",
        "print(\"Average 10 cv  specificity score :\",spec_cv_sum/10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}